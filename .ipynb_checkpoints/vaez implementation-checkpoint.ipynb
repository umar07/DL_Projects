{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umar/.local/lib/python3.6/site-packages/vaex/expression.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  result = np.array(result)\n",
      "/home/umar/.local/lib/python3.6/site-packages/vaex/expression.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  result = np.array(result)\n",
      "/home/umar/.local/lib/python3.6/site-packages/vaex/expression.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  result = np.array(result)\n",
      "/home/umar/.local/lib/python3.6/site-packages/vaex/expression.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  result = np.array(result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complex' 'frog_eye_leaf_spot' 'healthy' 'powdery_mildew' 'rust' 'scab']\n",
      "18632\n"
     ]
    }
   ],
   "source": [
    "#multi-label binarization\n",
    "df=vaex.open('/home/umar/ML-AI/datasets/plant-pathology-2021-fgvc8/train.csv')\n",
    "#df_copy=df\n",
    "df['labels']=df['labels'].apply(lambda x: x.split(' ')) #splitting multiple labels and getting a list\n",
    "\n",
    "label_train=[]\n",
    "for x in df['labels'].values:\n",
    "    #print(x)\n",
    "    label_train.append(x)\n",
    "\n",
    "label_train=np.array(label_train,dtype=object)\n",
    "\n",
    "mlb=MultiLabelBinarizer()\n",
    "label_train=mlb.fit_transform(label_train)\n",
    "\n",
    "label_test=np.array([[0,0,1,0,0,0],[0,0,1,0,0,0],[0,0,1,0,0,0]])\n",
    "\n",
    "print(mlb.classes_)\n",
    "print(len(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in training: 18632\n",
      "Number of files in testing: 3\n"
     ]
    }
   ],
   "source": [
    "# storing all the images for training dataset\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "dim=128\n",
    "\n",
    "path='/home/umar/ML-AI/datasets/plant-pathology-2021-fgvc8/'\n",
    "category=['train_images','test_images']\n",
    "\n",
    "train_path=os.path.join(path,category[0])\n",
    "test_path=os.path.join(path,category[1])\n",
    "\n",
    "x_train=[]\n",
    "x_test=[]\n",
    "\n",
    "for x in df['image'].values:\n",
    "    train_path_img=os.path.join(train_path,str(x))\n",
    "    img=cv2.imread(train_path_img,0) #grayscaling\n",
    "    img=cv2.resize(img,(dim,dim)) #resizing each image for uniformity\n",
    "    x_train.append([img])\n",
    "    \n",
    "for image in sorted(os.listdir(test_path)):\n",
    "    img=cv2.imread(os.path.join(test_path,image))\n",
    "    img=cv2.resize(img,(dim,dim)) #resizing each image for uniformity\n",
    "    x_test.append([img])\n",
    "\n",
    "x_train= np.array(x_train, dtype=\"float\") / 255.0\n",
    "x_test=np.array(x_test,dtype=\"float\") / 255.0\n",
    "\n",
    "print(\"Number of files in training:\",len(x_train)) \n",
    "print(\"Number of files in testing:\",len(x_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and validation data split\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,label_train,test_size=0.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reshape(-1,dim,dim,1)\n",
    "\n",
    "x_val=x_test.reshape(-1,dim,dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pretrained = InceptionV3(weights='imagenet', \n",
    "#                       include_top=False, \n",
    "#                       input_shape=(128,128,1))\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3,3), input_shape=(dim,dim,1), activation='relu',padding='same'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu',padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu',padding='same'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Conv2D(128, (3,3), activation='relu',padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1024, activation=\"relu\"))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(6,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 128, 128, 32)      320       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              33555456  \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 34,369,766\n",
      "Trainable params: 34,369,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=[tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 [==============================] - ETA: 0s - loss: 0.4984 - precision_2: 0.1310"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 9\n  y sizes: 3727\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-57c6d09526cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1128\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1131\u001b[0m           val_logs = self.evaluate(\n\u001b[1;32m   1132\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1527\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[1;32m   1528\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 9\n  y sizes: 3727\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "model_history=model.fit(x_train,y_train,batch_size=32,epochs=1,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3f68f06fb8d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_history' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(model_history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
